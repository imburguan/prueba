{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentação de Avaliação de frases do Contos de Fadas com as Valencias ANEW-BR ## \n",
    "\n",
    "O objetivo é obter frases que contenham palavras com expressões de sentimento para obter um banco de dados que possa ser usado para outros trabalhos na área de análise de emoções no texto narrativo em portugues brasileiro. \n",
    "\n",
    "Considerações Gerais\n",
    "Separação em sentenças\n",
    "Para ou análise de textos e mecanismos de uso comum para separar as frases com uma função split(). Neste processo, estamos trabalhando com processamento de linguagem natural e é por isso que usamos tokenize.\n",
    "Caracteres especiales del Portuguese\n",
    "\n",
    "Abaixo está um resumo.\n",
    "\n",
    "1. Tokenization for Portuguese\n",
    "Desde string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import sys  \n",
    "import os\n",
    "\n",
    "frase = \"O reino Pérsia ha séculos passados governado rei Nebul\"\n",
    "tokenizada = tokenize.word_tokenize(frase, language='portuguese')\n",
    "print tokenizada\n",
    "['O', 'reino', 'P\\xc3rsia', 'ha', 's\\xc3culos', 'passados', 'governado', 'rei', 'Nebul']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De um arquivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import sys  \n",
    "import os\n",
    "\n",
    "filepath = \"Name_Conto.txt\"\n",
    "if not os.path.isfile(filepath):\n",
    "    print(\"File path {} does not exist. Exiting...\".format(filepath))\n",
    "    sys.exit()\n",
    "\n",
    "bag_of_words = {}\n",
    "with open(filepath) as fp:\n",
    "    cnt = 0\n",
    "    for line in fp:\n",
    "        a = line.strip() \n",
    "        tokenizada = tokenize.word_tokenize(a, language='portuguese')\n",
    "    \n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Elimination of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_word_list = words[:]\n",
    "#print \"Words completa: \", words\n",
    "for word in words:\n",
    "    if word in stopwords.words('portuguese'): \n",
    "        filtered_word_list.remove(word) \n",
    "print \"Palavras despois do remove stopwords: \", filtered_word_list\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reescribir as novas frases em um arquivo diferente\n",
    "Para esse processo, um arquivo com uma extensão .txt é criado\n",
    "Para usar depois na análise de freqüências de palavras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range(len(filtered_word_list)):\n",
    "    archivo = open(\"Lineas_Name.txt\", \"r+\")\n",
    "    contenido = archivo.readline().decode('ISO 8859-1')\n",
    "    final_de_archivo = archivo.tell()\n",
    "    print \"palavras filtradas [i]:   \",filtered_word_list[i] \n",
    "    if (filtered_word_list[i] == \".\" or filtered_word_list[i] == \"\\n\" ):\n",
    "        archivo.write(filtered_word_list[i]+ '\\n')\n",
    "    else:\n",
    "        archivo.write(filtered_word_list[i]+ ' ')\n",
    "        archivo.seek(final_de_archivo)\n",
    "    archivo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Freqüências de palavras\n",
    "Um arquivo de extensão .csv é gravado. Donde se escribe 2 valores (palavra y numero de veces repetidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_words = filtered_word_list\n",
    "with open(\"Name-Frequency.csv\", \"w\") as the_file:\n",
    "    csv.register_dialect(\"custom\", delimiter=\"\\t\", skipinitialspace=True)\n",
    "    writer = csv.writer(the_file, dialect=\"custom\")\n",
    "    writer.writerow([\"palabras\", \"frecuencia\"])\n",
    "    for tup in sorted_words:\n",
    "        writer.writerow(tup)\n",
    "print (\"As 100 palavras mais frequentes sao: {}\".format(sorted_words[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é ordenado pelo maior número de frequências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_bag_of_word(bag_of_words, desc=False):\n",
    "    words = [(word, cnt) for word, cnt in bag_of_words.items()]\n",
    "    print \"O número total de palavras é: \",len (bag_of_words)\n",
    "return sorted(words, key=lambda x: x[1], reverse=desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, usamos o conto \"A rainha das Águas\" do autor Alberto Figueiredo Pimentel. Disponível em:\thttp://www.dominiopublico.gov.br/download/texto/bn000137.pdf\n",
    "\n",
    "Total de lineas: 89\n",
    "Palavras Tokenizadas: 2424\n",
    "O número total de palavras: 634\n",
    "\n",
    "De acordo com os pontos 3 e 4, dois arquivos são criados:\n",
    "\n",
    ".txt (Contém todas as linhas da história menos as palavras de stopwords)\n",
    "\n",
    "*\n",
    "\"A RAINHA DAS ÁGUAS \n",
    "O reino Pérsia há séculos passados governado rei Nebul .\n",
    "Esse rei , vivia feliz governando povo sabedoria , dia ficou cego .\n",
    "Mandou chamar todos médicos reino , todos curandeiros , todas feiticeiras , darem algum remédio curasse .\n",
    "Nada puderam conseguir .\n",
    "Já Nebul desanimado , conformado triste vida , dia , apareceu velhinha , pedindo esmola .\n",
    "Sabendo rei havia cegado , pediu ensinar remédio havia curar .\" \n",
    ".....\n",
    "\n",
    "\n",
    ".csv (Contém todas as palavras freqüentes e o número de repetições)\n",
    "\n",
    "palavras frequencia\n",
    ",\t241\n",
    ".\t93\n",
    "não\t21\n",
    "rei\t21\n",
    "príncipe  19\n",
    "o\t18\n",
    "oscar\t16\n",
    "nebul\t14\n",
    "à\t12\n",
    "reino\t11\n",
    "princesa  11\n",
    "mandou\t11\n",
    "ficou\t11\n",
    "água\t10\n",
    "fonte\t9\n",
    "filho\t9\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Estimativa das valências do sentimento positivo, neutro ou negativo.\n",
    "\n",
    "Segundo o artigo das Normas brasileiras para o Affective Norms for English Words, resume o siguente:\n",
    "\n",
    "* As palavras desagradáveis (valência < 4) como: assassino, miséria e tortura tem alerta alto (alerta > 5)\n",
    "* As palavras agradáveis (valência > 6) como: abraço, mãe e amigo tem alerta baixo (alerta < 5)\n",
    "* As palavras neutras (4 < valência < 6) como: reunão, passagem e martelo tem alerta baixo.\n",
    "\n",
    "Brazilian norms for the Affective Norms for English Words\t\t\t\t\n",
    "Affective Norms for English Words (ANEW-Br)\t\t\t\t\n",
    "            Valência\t\t  Alerta\t\n",
    "Palavra\t   Média\tDP\t   Média\tDP\n",
    "ABALADO\t    2,58\t1,81\t5,11\t2,82\n",
    "ABANDONADO\t1,85\t1,70\t5,48\t2,49\n",
    "ABDUÇÃO\t    5,00\t1,85\t3,36\t2,14\n",
    "ABELHAS\t    4,58\t2,45\t4,38\t2,83\n",
    "ABENÇOADO\t8,03\t1,84\t3,19\t3,03\n",
    "ABRAÇAR\t    8,63\t1,20\t4,30\t3,34\n",
    ".....\n",
    "são 1046 palavras. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para apresentar um exemplo, avaliaremos uma frase da história.\n",
    "\n",
    "*\"Já Nebul desanimado , conformado triste vida , dia , apareceu velhinha , pedindo esmola.\"\n",
    "\n",
    "Palavras: DESANIMADO, TRISTE, VIDA\n",
    "Promedio valencias 4.06666666667\n",
    "Polaridade: neutro\n",
    "\n",
    "*\"Mandou preparar nova esquadra quinhentos navios , porque supunha filho morrera guerra travara reino Águas , busca remédio cegueira.\"\n",
    "\n",
    "Palavras: GUERRA\n",
    "Promedio valencias 1.61\n",
    "Polaridade: negativo\n",
    "\n",
    "*\"Os dois irmãos aconselharam-lhe não continuasse viagem tempo perdido , pois país divertido , deixasse ficar ali\"\n",
    "\n",
    "Palavras: VIAGEM, TEMPO, PERDIDO, DIVERTIDO\n",
    "Promedio valencias 6.4275\n",
    "Polaridade: positivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Observações:\n",
    "\n",
    "Revisar as librerias de Python e Anaconda. (conflicto)\n",
    "Uma codificação do formato de entrada dá palavras porque não reconhoçe as palavras com acentos.\n",
    "Observa-se que o arquivo criado de linhas que ainda possui palavras que fornecem o ruído como exemplo: não, O, os...\n",
    "Na lista de freqüências de palavras pode ser usado para fazer algum gráfico para melhorar a apresentação de informações.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
