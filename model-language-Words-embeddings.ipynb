{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Languaging Modeling\n",
    "\n",
    "A detecção de padrões é o núcleo do mundo da linguagem natural.\n",
    "\n",
    "A detecção de padrões permite classificar documentos.\n",
    "\n",
    "A classificação dos documentos pode ser supervisionada ou não supervisionada.\n",
    "\n",
    "A aprendizagem supervisionada usa técnicas como:\n",
    "\n",
    "     1. Classificação de Naybe Bayes\n",
    "     2. Árvores de decisão (ID3 ou C45)\n",
    "     3. Tf-idf\n",
    "     4. Semântica latente\n",
    "     5. indeing LSI\n",
    "     6. SVM\n",
    "     7. Redes neurais\n",
    "         a) Algumas topologias são focadas no conceito de aprendizagem profunda\n",
    "         b) Algumas técnicas não usam redes para reconhecer padrões\n",
    "    \n",
    "Uma pergunta que tenho que responder é:\n",
    "\n",
    "Como sé que dos textos o palabras son semejantes? \n",
    "\n",
    "Pode-se interpretar que a extração da semântica identificará se as palavras são semelhantes.\n",
    "\n",
    "## 1. Extraindo Semântica: Para este propósito é necessário representar os documentos para que os algoritmos que usamos possam entendê-lo.\n",
    "\n",
    "A Semântica de informação não estruturada (fora de contexto), usa técnicas que criam modelos treinados a partir de um conjunto de documentos. Cada modelo cria uma representação vetorial. Este vetor contém uma série de valores, representando se um termo aparece no documento. Cada modelo tem um significado diferente do que é um termo que pode ser:\n",
    "\n",
    "     1. uma palavra\n",
    "     2. Uma frase\n",
    "     3. Um parágrafo, etc.\n",
    "\n",
    "Por exemplo, espaços vetoriais são usados para ver a distância entre vetores e saber se são semelhantes ou não (próximos ou distantes). Para saber quais palavras estão próximas, a \"distância de cosseno\" ou semelhança de cosseno será aplicada.\n",
    "\n",
    "Como existem muitas irregularidades lingüísticas, podemos aplicar operações vetoriais para extrair propriedades interessantes.\n",
    "\n",
    "Para representar textos temos 2 caminhos: a) localmente eb) continuamente\n",
    "\n",
    "    1. Localmente: palavras isoladas e representadas como um conjunto de termos índice ou palavras-chave (n-gramas, saco de palavras) NÃO TEM NENHUMA RELAÇÃO ENTRE TERMOS.\n",
    "\n",
    "    2. Continuamente: Eles são representados como matrizes ou vetores de junção e até mesmo nós (representações LSA ou LSI, LDS, LDA, distribuídas ou preditivas usando redes neurais). TOMA EM CONTA O CONTEXTO DAS PALAVRAS E A RELAÇÃO ENTRE ELES.\n",
    "    \n",
    "Para extrair a semântica, em nosso primer experimento tentaremos uma representação contínua chamada (distributed representations of words). E funcionam através do uso de um algoritmo para treinar um conjunto de vetores de comprimento fixo, denso e contínuo, com base em um grande corpus de textos.\n",
    "\n",
    "Essa representação aprende representações vetoriais de palavras, ou seja, teremos um espaço multidimensional no qual uma palavra é representada como um vetor. A representação do espaço vetorial de palavras fornece uma projeção em que palavras com significados semelhantes são agrupadas localmente no espaço.\n",
    "\n",
    "   1. Esses vetores são capazes de extrair características relevantes, tais como: i) propriedades sintáticas, ii) propriedades semânticas das palavras (Turian 2010) Referenciar bem.\n",
    "\n",
    "   2. O outro é o aprendizado de máquina é feito com dados de entrada NÃO LABELADOS, isto é, NÃO É SUPERVISIONADO.\n",
    "    \n",
    "Esses vetores são usados como entrada para muitas aplicações de PNL ou Machine Learning.\n",
    "Para aplicar essa técnica, usamos a ferramenta Word2Vec (Mikolov Google 2013) https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf, que usa como entrada um corpus de textos ou qualquer documentação. Obter vetores de saída representando as palavras.\n",
    "\n",
    "A arquitetura na qual o Word2vec se baseia são as redes neurais para aprender essas representações. Você também pode obter vetores que representam frases, parágrafos ou mesmo documentos completos (Le & Mikolov 2014)\n",
    "\n",
    "No nosso caso, vamos nos concentrar no nível de palavras.\n",
    "\n",
    "Para a implementação do Python, precisaremos da biblioteca gesim.\n",
    "\n",
    "Word2Vec Tem dois tipos de algoritmos, como: Skip-gram y Bags-of-words (CBOW).\n",
    "\n",
    "    1.Para skip-gram. O modelo é a palavra objetiva, enquanto as saídas são as palavras que cercam a palavra objetiva.\n",
    "    \n",
    "    Exemplo: \n",
    "        I have a cute dog\n",
    "    Input : \"a\" \n",
    "    Output: I have cute dog\n",
    "    Tamanho da janela: 5\n",
    "   2. Continuos Bag-of-Word CBOW. O modelo prevê a palavra atual a partir de uma janela de palavras de contexto adjacentes. A ordem das palavras de contexto não influencia na previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vamos hacer un ejemplo con 5 frases: \n",
    "    1. \"Era uma vez uma menininha chamada Alice\"\n",
    "    2. \"Um dia ela teve um sonho muito estranho\"\n",
    "    3. \"Voce gostaria de saber como foi o sonho dela?\"\n",
    "    4. \"Bem, vamos comecar pelo comeco!\"\n",
    "    5. \"A primeira coisa que ela viu no tal sonho foi um Coelho Branco muito apressado e correndo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sentences = [['Era', 'uma', 'vez', 'uma', 'menininha', 'chamada', 'Alice'],\n",
    "\t\t\t['Um', 'dia', 'ela', 'teve', 'um','sonho','muito', 'estranho'],\n",
    "\t\t\t['Voce', 'gostaria', 'de', 'saber', 'como', 'foi', 'o', 'sonho', 'dela'],\n",
    "\t\t\t['Bem', 'vamos', 'comecar', 'pelo', 'comeco'],\n",
    "\t\t\t['A', 'primeira', 'coisa', 'que', 'ela', 'viu', 'no', 'tal', 'sonho', 'foi', 'um', 'Coelho', 'Branco', 'muito', 'apressado', 'e', 'correndo']]\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['Alice'])\n",
    "\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "#Podemos recuperar todos os vetores de um modelo treinado da seguinte maneira:\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "#adicionando a criação de um modelo PCA bidimensional da palavra vetores usando a classe PCA scikit-learn da seguinte maneira.\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "print (result)\n",
    "\n",
    "#A adição à plotagem pode ser plotada usando o matplotlib da seguinte maneira, extraindo as duas dimensões como coordenadas x e y.\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "#plt.show()\n",
    "\n",
    "#Podemos ir um passo além e anotar os pontos no gráfico com as próprias palavras.\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tplt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#pasos seguidos em https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "Archivo en mi maquina las primeras lineas eran de un ejemplo que no funciono para NILC porque los enlaces del corpus estan rotos\n",
    "Mi aquivo python se llama ...... uspW2Vprueba1.py que tiene un ejemplo con 5 frases de mis cuentos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
